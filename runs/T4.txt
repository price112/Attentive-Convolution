#!/bin/bash
#SBATCH --job-name=666
#SBATCH --account=project_462001033
#SBATCH --nodes=16
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=48
#SBATCH --mem=480G
#SBATCH --partition=standard-g
#SBATCH --time=1-24:00:00

module purge
module load LUMI/24.03 partition/G rocm/6.0.3
export OMP_NUM_THREADS=1

export PATH="/users/conda/pytorch/bin:$PATH"

export NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3
export NCCL_NET_GDR_LEVEL=PHB

export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=29500

srun torchrun \
  --nnodes=$SLURM_JOB_NUM_NODES \
  --nproc_per_node=8 \
  --rdzv_id=\$SLURM_JOB_ID \
  --rdzv_backend=c10d \
  --rdzv_endpoint="$MASTER_ADDR:$MASTER_PORT" \
   train.py \
    --batch_size 32 \
    --lr 0.004 \
    --epochs 300 \
    --model AttNet_T4 \
    --input_size 224 \
    --drop_path_rate 0.5 \
    --weight_decay 0.05 \
    --num_workers 8 \
    --data_path ../ImageNet_train_lmdb/ImageNet_lmdb \
    --output_dir AttNet_T4 \
    --color-jitter 0.4 \
    --reprob 0.25 \
    --mixup 0.8 \
    --cutmix 1.0 \
    --pin_mem \
    --aa rand-m9-mstd0.5-inc1 \
