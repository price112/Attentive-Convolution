#!/bin/bash
#SBATCH --job-name=666
#SBATCH --account=project_462001033
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=8
#SBATCH --cpus-per-task=48
#SBATCH --mem=480G
#SBATCH --partition=standard-g
#SBATCH --time=1-24:00:00

module purge
module load LUMI/24.03 partition/G rocm/6.0.3
export OMP_NUM_THREADS=1

export PATH="/users/conda/torch_old/bin:$PATH"

export NCCL_SOCKET_IFNAME=hsn0,hsn1,hsn2,hsn3
export NCCL_NET_GDR_LEVEL=PHB

export MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)
export MASTER_PORT=29500

srun torchrun \
  --nnodes=$SLURM_JOB_NUM_NODES \
  --nproc_per_node=8 \
  --rdzv_id=\$SLURM_JOB_ID \
  --rdzv_backend=c10d \
  --rdzv_endpoint="$MASTER_ADDR:$MASTER_PORT" \
   train.py \
    --model "SiT-S/2-ATConv" \
    --global-batch-size "128" \
    --image-size "256" \
    --data-path  "../ffhq/" \
    --results-dir "" \
    --amp-bf16 \
    --num-classes 1 \
    --cfg-scale 1.0 \
    --wandb \
    --sample-every 10000 \
    --ckpt-every 50000 \
    --tips "default" \
